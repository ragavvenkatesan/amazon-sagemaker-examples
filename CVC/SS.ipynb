{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Semantic Segmentation\n",
    "\n",
    "## Table of contents\n",
    "1. [Introduction](#Introduction)\n",
    "   1. [Types of Segmentation](#Types-of-Segmentation)\n",
    "   2. [Deep Learning for Semantic Segmentation](#Deep-Learning-for-Semantic-Segmentation)\n",
    "     * [Repurposing existing VOC SOTA](#Repurpose-the-existing-ImageNet-SOTA)\n",
    "     * [Fully Convolutional Networks](#Fully-convolutional-networks)\n",
    "2. [Setup](#Setup)\n",
    "3. [Data Preparation](#Data-Preparation)\n",
    "  1. [Download data](#Download-data)\n",
    "  2. [Setup Data](#Setup-data)\n",
    "  3. [Upload to S3](#Upload-to-S3)\n",
    "4. [Training](#Training)\n",
    "5. [Hosting](#Hosting)\n",
    "6. [Inference](#Inference)\n",
    "\n",
    "___\n",
    "\n",
    "## Introduction\n",
    "<div align=\"center\"><p>\n",
    "   <img src=\"figures/imageclassification.png\" alt=\"Image Classification\" style=\"width: 370px;\"/><br>\n",
    "   <img src=\"figures/objectdetection.png\" alt=\"Object Detection\" style=\"width: 205px;\"/><br>\n",
    "   <img src=\"figures/semanticsegmentation.png\" alt=\"Semantic Segmentation\" style=\"width: 460px;\"/> <br>   \n",
    "</p></div>\n",
    "\n",
    "1. **Image Classification**: One integer per image.\n",
    "2. **Object Detection**: An integer for class and four more for bounding box locations.\n",
    "3. **Semantic Segmentation**: One integer per pixel of the image.\n",
    "___\n",
    "\n",
    "### Types of Segmentation\n",
    "<div align=\"center\"><p>\n",
    "   <img src=\"figures/typesofseg.jpg\" alt=\"Types of segments\" style=\"width: 1000px;\"/>\n",
    "   <img src=\"figures/types_of_segmentation.png\" alt=\"Types of segmentation\" style=\"width: 1000px;\"/>   \n",
    "</p></div>\n",
    "\n",
    "1. **Segmentation**: Group different parts of the image. \n",
    "2. **Semantic Segmentation**: Each group is assigned a class label.\n",
    "3. **Instance Segmentation**: Segment within class.\n",
    "___\n",
    "\n",
    "### Deep Learning for Semantic Segmentation\n",
    "<div align=\"center\"><p>\n",
    "   <img src=\"figures/pipeline.png\" alt=\"Deep learning pipeline\" style=\"width: 1000px;\"/>\n",
    "</p></div>\n",
    "\n",
    "A typical deep network for segmentation contains two parts:\n",
    "- The feature extractor or *backbone*.\n",
    "- The upsampler, that takes the feature from the latent space and projects them to the same shape as the image itself.\n",
    "\n",
    "Two points of note here:\n",
    "1. The features at the latent space must retain *spatial information*.\n",
    "2. The upsampled output layer should produce one signal-per-location.\n",
    "\n",
    "#### Repurpose the existing ImageNet SOTA\n",
    "<div align=\"center\"><p>\n",
    "   <img src=\"figures/pretrained-feature.png\" alt=\"SOTA to segmentation nets\" style=\"width: 800px;\"/>\n",
    "</p></div>\n",
    "\n",
    "Maintaining the *spatial infromation* implies that we can't have fully-connected layers. There are two general solutions available for this problem.\n",
    "1. Do not use fully-connected layers.\n",
    "2. Repurpose the fully-connected layers and $1 \\times 1$ convolutional layers.\n",
    "Both these solution imply that the size of the output is related to the size of the input and we maintain *spatial information* in the features. \n",
    "\n",
    "We can then make use of either interpolation or transposed convolution techniques to reconstruct one signal-per-pixel output. The last layer is a $N$-class classifier one for each class.\n",
    "\n",
    "#### Fully convolutional networks\n",
    "<div align=\"center\"><p>\n",
    "   <img src=\"figures/fcn.png\" alt=\"SOTA to segmentation nets\" style=\"width: 800px;\"/>\n",
    "</p></div>\n",
    "\n",
    "### Amazon SageMaker Semantic Segmentation\n",
    "This notebook is an end-to-end example introducing the Amazon SageMaker Semantic Segmentation algorithm and many of the features it supports. \n",
    "\n",
    "In this demo, we will demonstrate how to train and host a semantic segmentation model using\n",
    "\n",
    " - the Fully-convolutional network ([FCN](https://arxiv.org/abs/1605.06211)) algorithm and\n",
    " - the [Pascal VOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/). \n",
    "Other algorithm options that are available:\n",
    "\n",
    "- Pyramid Scene Parsing Network ([PSP](https://arxiv.org/abs/1612.01105))\n",
    "- [Deeplab-v3](https://arxiv.org/abs/1706.05587).\n",
    "\n",
    "\n",
    "Also demonstrated in this session:\n",
    "\n",
    "- Dataset curation and setup.\n",
    "\n",
    "___\n",
    "\n",
    "## AWS Setup\n",
    "To train the Semantic Segmentation algorithm on Amazon SageMaker, we need to setup and authenticate the use of AWS services. To begin with, we need an AWS account role with SageMaker access. This role that is used to give SageMaker access to your data in S3 can automatically be obtained from the role used to start the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::686482943557:role/service-role/AmazonSageMaker-ExecutionRole-20190108T194620\n",
      "685385470294.dkr.ecr.eu-west-1.amazonaws.com/semantic-segmentation:latest\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the S3 bucket that is used to store training data and the trained model artifacts. In this notebook, we use the default bucket that comes with Sagemaker. However, you can also create a bucket and use that bucket instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-eu-west-1-686482943557\n"
     ]
    }
   ],
   "source": [
    "bucket = sess.default_bucket()  \n",
    "prefix = 'semantic-segmentation-demo'\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) is a popular computer vision dataset which is used for annual semantic segmentation challenges from 2005 to 2012. The dataset has 1464 training and 1449 validation images with 21 classes. Examples of the segmentation dataset can be seen in the [Pascal VOC Dataset page](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/index.html). The classes are as follows:\n",
    "\n",
    "| Label Id |     Class     |\n",
    "|:--------:|:-------------:|\n",
    "|     0    |   Background  |\n",
    "|     1    |   Aeroplane   |\n",
    "|     2    |    Bicycle    |\n",
    "|     3    |      Bird     |\n",
    "|     4    |      Boat     |\n",
    "|    5     |     Bottle    |\n",
    "|     6    |      Bus      |\n",
    "|     7    |      Car      |\n",
    "|     8    |      Cat      |\n",
    "|     9    |     Chair     |\n",
    "|    10    |      Cow      |\n",
    "|    11    |  Dining Table |\n",
    "|    12    |      Dog      |\n",
    "|    13    |     Horse     |\n",
    "|    14    |   Motorbike   |\n",
    "|    15    |     Person    |\n",
    "|    16    |  Potted Plant |\n",
    "|    17    |     Sheep     |\n",
    "|    18    |      Sofa     |\n",
    "|    19    |     Train     |\n",
    "|    20    |  TV / Monitor |\n",
    "|    255   | Hole / Ignore |\n",
    "\n",
    "In this notebook, we will use the data sets from 2012. While using the Pascal VOC dataset, please be aware of the  usage rights:\n",
    "\"The VOC data includes images obtained from the \"flickr\" website. Use of these images must respect the corresponding terms of use: \n",
    "* \"flickr\" terms of use (https://www.flickr.com/help/terms)\"\n",
    "<div align=\"center\"><p>\n",
    "<img src=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/images/21.jpg\" width=\"200\"/>\n",
    "<img src=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/images/21_class.png\" width=\"200\"/>\n",
    "<img src=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/images/18.jpg\" width=\"219\"/>\n",
    "<img src=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/images/18_class.png\" width=\"219\"/>    \n",
    "</p></div>\n",
    "\n",
    "### Download data\n",
    "Let us download the Pascal VOC datasets from VOC 2012.\n",
    "\n",
    "If this notebook was run before, you may have downloaded some data and set them up. If you have done this section properly, do not run the cell below as it will download the data all over again. If you have downloaded and want to re-download and reprocess the data, run the cell below to clean the previous download.\n",
    "\n",
    "If you have already downloaded and setup the data, you do not need to run the following cells in this section. You can instead use the previous S3 bucket. If not clean up directories created from the previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘test_reshaped.jpg’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -rf train\n",
    "!rm -rf train_annotation\n",
    "!rm -rf validation\n",
    "!rm -rf validation_annotation\n",
    "!rm -rf VOCdevkit\n",
    "!rm test.jpg\n",
    "!rm test_reshaped.jpg\n",
    "!rm train_label_map.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-02-04 14:44:22--  http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
      "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1999639040 (1.9G) [application/x-tar]\n",
      "Saving to: ‘/tmp/VOCtrainval_11-May-2012.tar’\n",
      "\n",
      "VOCtrainval_11-May- 100%[===================>]   1.86G  60.9MB/s    in 28s     \n",
      "\n",
      "2019-02-04 14:44:50 (69.3 MB/s) - ‘/tmp/VOCtrainval_11-May-2012.tar’ saved [1999639040/1999639040]\n",
      "\n",
      "CPU times: user 1.06 s, sys: 279 ms, total: 1.34 s\n",
      "Wall time: 50.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Download the dataset\n",
    "!wget -P /tmp http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar    \n",
    "# # Extract the data.\n",
    "!tar -xf /tmp/VOCtrainval_11-May-2012.tar && rm /tmp/VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data\n",
    "\n",
    "Move the images into appropriate directory structure as described in the [documentation](link-to-documentation). This is quite simply, moving the \n",
    "training images to a `train` directory ans so on. Fortunately, the dataset's annotations are already named in sync with the image names, satisfying one requirement of the Amazon SageMaker Semantic Segmentation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainval.txt', 'train.txt', 'val.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# We want to read the images in these files.\n",
    "os.listdir('VOCdevkit/VOC2012/ImageSets/Segmentation')\n",
    "# !tail VOCdevkit/VOC2012/ImageSets/Segmentation/val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Create directory structure mimicing the s3 bucket where data is to be dumped.\n",
    "VOC2012 = 'VOCdevkit/VOC2012'\n",
    "os.makedirs('train', exist_ok=True)\n",
    "os.makedirs('validation', exist_ok=True)\n",
    "os.makedirs('train_annotation', exist_ok=True)\n",
    "os.makedirs('validation_annotation', exist_ok=True)\n",
    "\n",
    "# Create a list of all training images.\n",
    "filename = VOC2012+'/ImageSets/Segmentation/train.txt'\n",
    "with open(filename) as f:\n",
    "    train_list = f.read().splitlines() \n",
    "\n",
    "# Create a list of all validation images.\n",
    "filename = VOC2012+'/ImageSets/Segmentation/val.txt'\n",
    "with open(filename) as f:\n",
    "    val_list = f.read().splitlines() \n",
    "\n",
    "# Move the jpg images in training list to train directory and png images to train_annotation directory.\n",
    "for i in train_list:\n",
    "    shutil.copy2(VOC2012+'/JPEGImages/'+i+'.jpg', 'train/')\n",
    "    shutil.copy2(VOC2012+'/SegmentationClass/'+i+'.png','train_annotation/' )\n",
    "\n",
    "# Move the jpg images in validation list to validation directory and png images to validation_annotation directory.\n",
    "for i in val_list:\n",
    "    shutil.copy2(VOC2012+'/JPEGImages/'+i+'.jpg', 'validation/')\n",
    "    shutil.copy2(VOC2012+'/SegmentationClass/'+i+'.png','validation_annotation/' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if the move was completed correctly. If it was done correctly, the number of jpeg images in `train` and png images in `train_annotation` must be the same, and so in validation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Num Train Images = 1464\n",
      " Num Validation Images = 1449\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "num_training_samples=len(glob.glob1('train',\"*.jpg\"))\n",
    "\n",
    "print ( ' Num Train Images = ' + str(num_training_samples))\n",
    "assert num_training_samples == len(glob.glob1('train_annotation',\"*.png\"))\n",
    "\n",
    "print ( ' Num Validation Images = ' + str(len(glob.glob1('validation',\"*.jpg\"))))\n",
    "assert len(glob.glob1('validation',\"*.jpg\")) == len(glob.glob1('validation_annotation',\"*.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now move our prepared datset to the S3 bucket that we decided to use in this notebook earlier. Notice the following directory structure that is used.\n",
    "\n",
    "```bash\n",
    "root \n",
    "|-train/\n",
    "|-train_annotation/\n",
    "|-validation/\n",
    "|-validation_annotation/\n",
    "\n",
    "```\n",
    "- All the images in the `_annotation` directory are all indexed PNG files. \n",
    "- This implies that the metadata (color mapping modes) of the files contain information on how to map the indices to colors and vice versa. \n",
    "- The integers are also within `[0, 1 ... c-1, 255]`  for a `c` class segmentation problem, with `255` as 'hole' or 'ignore' class. \n",
    "- We allow any mode that is a [recognized standard](https://pillow.readthedocs.io/en/3.0.x/handbook/concepts.html#concept-modes) as long as they are read as integers.\n",
    "\n",
    "#### Label Maps\n",
    "\n",
    "While we recommend the format with default color mapping modes such as PASCAL, we also allow the customers to specify their own label maps. Refer to the [documentation](Permalink-to-label-map-documentation-section) for more details. The label map for the PASCAL VOC dataset, is the default (which we use incase no label maps are provided): \n",
    "```json\n",
    "{\n",
    "    \"scale\": 1\n",
    "}```\n",
    "This essentially tells us to simply use the images as read as integers as labels directly. Since we are using PASCAL dataset, let us create (recreate the default just for demonstration) label map for training channel and let the algorithm use the default (which is exactly the same for the validation channel). If `label_map` is used, please pass it to the label_map channel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "label_map = { \"scale\": 1 }\n",
    "with open('train_label_map.json', 'w') as lm_fname:\n",
    "    json.dump(label_map, lm_fname)\n",
    "    \n",
    "# Create channel names for the s3 bucket.\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "train_annotation_channel = prefix + '/train_annotation'\n",
    "validation_annotation_channel = prefix + '/validation_annotation'\n",
    "# label_map_channel = prefix + '/label_map'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to S3\n",
    "Let us now upload our dataset including our label map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.1 s, sys: 2.39 s, total: 38.5 s\n",
      "Wall time: 4min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# upload the appropraite directory up to s3 respectively for all directories.\n",
    "sess.upload_data(path='train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='validation', bucket=bucket, key_prefix=validation_channel)\n",
    "sess.upload_data(path='train_annotation', bucket=bucket, key_prefix=train_annotation_channel)\n",
    "sess.upload_data(path='validation_annotation', bucket=bucket, key_prefix=validation_annotation_channel)\n",
    "# sess.upload_data(path='train_label_map.json', bucket=bucket, key_prefix=label_map_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job. Let us use another channel in the same S3 bucket for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-west-1-686482943557/semantic-segmentation-demo/output\n"
     ]
    }
   ],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print(s3_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our segmentation algorithm. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job. Let us name our training job as `ss-notebook-demo`. Let us also use a nice-and-fast GPU instance (`ml.p3.2xlarge`) to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sagemaker estimator object.\n",
    "ss_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count = 1, \n",
    "                                         train_instance_type = 'ml.p3.2xlarge',\n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000,\n",
    "                                         output_path = s3_output_location,\n",
    "                                         base_job_name = 'ss-notebook-demo',\n",
    "                                         sagemaker_session = sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantic segmentation algorithm at its core has two compoenents.\n",
    "\n",
    "**An encoder or backbone network.**\n",
    "  - The encoder or backbone network is typically a regular convolutional neural network that may or maynot have had their layers pre-trained on [ImageNet](http://www.image-net.org/). \n",
    "  - The Amazon SageMaker Semantic Segmentation algorithm comes with two choices of pre-trained or to be trained-from-scratch backbone networks ([ResNets](https://arxiv.org/abs/1512.03385) 50 or 101). \n",
    "  \n",
    "**A decoder or algorithm network.**\n",
    "  - The decoder is a network that picks up the outputs of one or many layers from the backbone and reconstructs the segmentation mask from it. \n",
    "  - Amazon SageMaker Semantic Segmentation algorithm comes with a choice of the [Fully-convolutional network (FCN)](https://arxiv.org/abs/1605.06211) or the [Pyramid scene parsing (PSP) network](https://arxiv.org/abs/1612.01105).\n",
    "\n",
    "The algorithm also has ample options for hyperparameters that help configure the training job. The next step in our training, is to setup these networks and hyperparameters along with data channels for training the model. Consider the following example definition of hyperparameters. See the SageMaker Semantic Segmentation [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html) for more details on the hyperparameters.\n",
    "\n",
    "One of the hyperparameters here for instance is the `epochs`. This defines how many passes of the dataset we iterate over and determines that training time of the algorithm. For the sake of demonstration let us run only `10` epochs. Based on our tests, train the model for `30` epochs with similar settings should give us 'reasonable' segmentation results on the Pascal VOC data. For the most part, we will stick to using the simplest of settings. For more information on the hyperparameters of this algorithm, refer to the [documentation](perma-link-to-hyperparameter-section-in-documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hyperparameters \n",
    "ss_model.set_hyperparameters(backbone='resnet-50', # This is the encoder. Other option is resnet-50\n",
    "                             algorithm='fcn', # This is the decoder. Other option is 'psp' and 'deeplab'                             \n",
    "                             use_pretrained_model='True', # Use the pre-trained model.\n",
    "                             crop_size=240, # Size of image random crop.                             \n",
    "                             num_classes=21, # Pascal has 21 classes. This is a mandatory parameter.\n",
    "                             epochs=10, # Number of epochs to run.\n",
    "                             learning_rate=0.0001,                             \n",
    "                             optimizer='rmsprop', # Other options include 'adam', 'rmsprop', 'nag', 'adagrad'.\n",
    "                             lr_scheduler='poly', # Other options include 'cosine' and 'step'.                           \n",
    "                             mini_batch_size=16, # Setup some mini batch size.\n",
    "                             validation_mini_batch_size=16,\n",
    "                             early_stopping=True, # Turn on early stopping. If OFF, other early stopping parameters are ignored.\n",
    "                             early_stopping_patience=2, # Tolerate these many epochs if the mIoU doens't increase.\n",
    "                             early_stopping_min_epochs=10, # No matter what, run these many number of epochs.                             \n",
    "                             num_training_samples=num_training_samples) # This is a mandatory parameter, 1464 in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyperparameters are setup, let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. These objects are then put in a simple dictionary, which the algorithm uses to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full bucket names\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)\n",
    "s3_train_annotation = 's3://{}/{}'.format(bucket, train_annotation_channel)\n",
    "s3_validation_annotation = 's3://{}/{}'.format(bucket, validation_annotation_channel)\n",
    "\n",
    "distribution = 'FullyReplicated'\n",
    "# Create sagemaker s3_input objects\n",
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution=distribution, \n",
    "                                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution=distribution, \n",
    "                                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "train_annotation = sagemaker.session.s3_input(s3_train_annotation, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "validation_annotation = sagemaker.session.s3_input(s3_validation_annotation, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data,\n",
    "                 'train_annotation': train_annotation, \n",
    "                 'validation_annotation':validation_annotation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our `Estimator` object, we have set the hyperparameters for this object and we have our data channels linked with the algorithm. The only remaining thing to do is to train the algorithm. The following command will train the algorithm. Training the algorithm involves a few steps. Firstly, the instances that we requested while creating the `Estimator` classes are provisioned and are setup with the appropriate libraries. Then, the data from our channels are downloaded into the instance. Once this is done, the training job begins. The provisioning and data downloading will take time, depending on the size of the data and the availability of the type of instances. Therefore it might be a few minutes before we start getting data logs for our training jobs. The data logs will also print out training loss on the training data, which is the pixel-wise cross-entropy loss as described in the algorithm papers. The data logs will also print out pixel-wise label accuracy and mean intersection-over-union (mIoU) on the validation data after a run of the dataset once or one epoch. These metrics measure the quality of the model under training.\n",
    "\n",
    "Once the job has finished a \"Job complete\" message will be printed. The trained model can be found in the S3 bucket that was setup as `output_path` in the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: ss-notebook-demo-2019-02-04-14-53-21-366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-04 14:53:21 Starting - Starting the training job...\n",
      "2019-02-04 14:53:23 Starting - Launching requested ML instances......\n",
      "2019-02-04 14:54:25 Starting - Preparing the instances for training......\n",
      "2019-02-04 14:55:24 Downloading - Downloading input data...\n",
      "2019-02-04 14:56:11 Training - Training image download completed. Training in progress.\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:14 INFO 140104354305856] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'gamma2': u'0.9', u'gamma1': u'0.9', u'early_stopping_min_epochs': u'5', u'epochs': u'10', u'_workers': u'16', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0001', u'crop_size': u'240', u'use_pretrained_model': u'True', u'_aux_weight': u'0.5', u'_hybrid': u'False', u'_augmentation_type': u'default', u'lr_scheduler': u'poly', u'early_stopping_patience': u'4', u'momentum': u'0.9', u'optimizer': u'sgd', u'early_stopping_tolerance': u'0.0', u'learning_rate': u'0.001', u'backbone': u'resnet-50', u'validation_mini_batch_size': u'16', u'_aux_loss': u'True', u'mini_batch_size': u'16', u'_precision_dtype': u'float32', u'early_stopping': u'False', u'algorithm': u'fcn', u'_logging_frequency': u'20', u'num_training_samples': u'8', u'_kvstore': u'device', u'_syncbn': u'False'}\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:14 INFO 140104354305856] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.0001', u'optimizer': u'rmsprop', u'algorithm': u'fcn', u'lr_scheduler': u'poly', u'use_pretrained_model': u'True', u'backbone': u'resnet-50', u'early_stopping_min_epochs': u'10', u'epochs': u'10', u'validation_mini_batch_size': u'16', u'num_training_samples': u'1464', u'num_classes': u'21', u'mini_batch_size': u'16', u'early_stopping_patience': u'2', u'early_stopping': u'True', u'crop_size': u'240'}\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:14 INFO 140104354305856] Final configuration: {u'gamma2': u'0.9', u'gamma1': u'0.9', u'early_stopping_min_epochs': u'10', u'epochs': u'10', u'_workers': u'16', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0001', u'crop_size': u'240', u'use_pretrained_model': u'True', u'_aux_weight': u'0.5', u'_hybrid': u'False', u'_augmentation_type': u'default', u'lr_scheduler': u'poly', u'num_classes': u'21', u'early_stopping_patience': u'2', u'momentum': u'0.9', u'optimizer': u'rmsprop', u'early_stopping_tolerance': u'0.0', u'learning_rate': u'0.0001', u'backbone': u'resnet-50', u'validation_mini_batch_size': u'16', u'_aux_loss': u'True', u'mini_batch_size': u'16', u'_precision_dtype': u'float32', u'early_stopping': u'True', u'algorithm': u'fcn', u'_logging_frequency': u'20', u'num_training_samples': u'1464', u'_kvstore': u'device', u'_syncbn': u'False'}\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:14 INFO 140104354305856] Using default worker.\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:14 INFO 140104354305856] Loaded iterator creator application/x-image for content type ('application/x-image', '1.0')\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:14 INFO 140104354305856] Loaded iterator creator application/x-recordio for content type ('application/x-recordio', '1.0')\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:14 INFO 140104354305856] Loaded iterator creator image/png for content type ('image/png', '1.0')\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:14 INFO 140104354305856] Loaded iterator creator application/json for content type ('application/json', '1.0')\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:14 INFO 140104354305856] Loaded iterator creator image/jpeg for content type ('image/jpeg', '1.0')\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:14 WARNING 140104354305856] /opt/ml/input/data/train/_annotation is not a readable image file\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:15 WARNING 140104354305856] label maps not provided, using defaults.\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:15 INFO 140104354305856] #label_map train :{'scale': 1}\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:15 WARNING 140104354305856] /opt/ml/input/data/validation/_annotation is not a readable image file\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:15 WARNING 140104354305856] label maps not provided, using defaults.\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:15 INFO 140104354305856] #label_map validation :{'scale': 1}\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:15 INFO 140104354305856] nvidia-smi took: 0.0754790306091 secs to identify 1 gpus\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:15 INFO 140104354305856] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:15 INFO 140104354305856] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[31mModel file is not found. Downloading.\u001b[0m\n",
      "\u001b[31mDownloading /root/.mxnet/models/resnet50_v1s-25a187fa.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/resnet50_v1s-25a187fa.zip...\u001b[0m\n",
      "\u001b[31m#015  0%|          | 0/57417 [00:00<?, ?KB/s]#015  0%|          | 34/57417 [00:00<03:44, 255.72KB/s]#015  0%|          | 101/57417 [00:00<02:27, 387.68KB/s]#015  0%|          | 229/57417 [00:00<01:37, 584.32KB/s]#015  1%|          | 501/57417 [00:00<00:59, 955.57KB/s]#015  2%|1         | 965/57417 [00:00<00:38, 1471.56KB/s]#015  3%|3         | 1925/57417 [00:00<00:22, 2442.17KB/s]#015  7%|6         | 3845/57417 [00:00<00:12, 4183.94KB/s]#015  9%|9         | 5365/57417 [00:01<00:10, 5106.78KB/s]#015 17%|#6        | 9545/57417 [00:01<00:05, 8295.80KB/s]#015 22%|##1       | 12440/57417 [00:01<00:04, 9466.67KB/s]#015 28%|##8       | 16265/57417 [00:01<00:03, 11253.28KB/s]#015 34%|###3      | 19339/57417 [00:01<00:03, 12247.79KB/s]#015 40%|####      | 23149/57417 [00:01<00:02, 13543.71KB/s]#015 45%|####5     | 26091/57417 [00:01<00:02, 14169.70KB/s]#015 50%|####9     | 28447/57417 [00:01<00:02, 14392.75KB/s]#015 56%|#####5    | 32126/57417 [00:02<00:01, 15272.22KB/s]#015 60%|######    | 34692/57417 [00:02<00:01, 15517.65KB/s]#015 64%|######4   | 36947/57417 [00:02<00:01, 15446.52KB/s]#015 71%|#######   | 40659/57417 [00:02<00:01, 16205.29KB/s]#015 78%|#######7  | 44740/57417 [00:02<00:00, 17148.34KB/s]#015 85%|########4 | 48798/57417 [00:02<00:00, 18013.23KB/s]#015 91%|######### | 52029/57417 [00:02<00:00, 18314.07KB/s]#015 96%|#########5| 55025/57417 [00:02<00:00, 18599.37KB/s]#01557418KB [00:03, 18986.52KB/s]                           \u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/mxnet/gluon/block.py:421: UserWarning: load_params is deprecated. Please use load_parameters.\n",
      "  warnings.warn(\"load_params is deprecated. Please use load_parameters.\")\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1549292191.850627, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\"}, \"StartTime\": 1549292191.850515}\n",
      "\u001b[0m\n",
      "\u001b[31m[14:56:33] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x.178.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:109: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:45 INFO 140104354305856] #progress_notice. epoch: 0, iterations: 20 speed: 54.4619761973 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:51 INFO 140104354305856] #progress_notice. epoch: 0, iterations: 40 speed: 54.5477233601 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:56:57 INFO 140104354305856] #progress_notice. epoch: 0, iterations: 60 speed: 54.8822958869 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:57:03 INFO 140104354305856] #progress_notice. epoch: 0, iterations: 80 speed: 55.5383855278 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:57:06 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 0, train loss: 1.5844667110141817 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:57:06 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 0, train throughput: 44.5820297727 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:57:21 INFO 140104354305856] #progress_notice. epoch: 0, iterations: 20 speed: 23.7821628987 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:57:35 INFO 140104354305856] #progress_notice. epoch: 0, iterations: 40 speed: 23.3679304807 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:57:49 INFO 140104354305856] #progress_notice. epoch: 0, iterations: 60 speed: 23.6441930873 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:58:02 INFO 140104354305856] #progress_notice. epoch: 0, iterations: 80 speed: 23.5830974749 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:58:09 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 0, validation pixel_accuracy: 0.8229120028117166 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:58:09 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 0, validation mIOU: 0.43909800028598756 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:58:09 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 0, validation throughput: 23.437315625 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:58:09 INFO 140104354305856] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:58:09 INFO 140104354305856] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:58:09 INFO 140104354305856] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1549292289.994742, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 0}, \"StartTime\": 1549292191.850873}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:58:18 INFO 140104354305856] #progress_notice. epoch: 1, iterations: 20 speed: 54.3161152677 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:58:24 INFO 140104354305856] #progress_notice. epoch: 1, iterations: 40 speed: 54.3004253646 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:58:30 INFO 140104354305856] #progress_notice. epoch: 1, iterations: 60 speed: 54.6916677533 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:58:36 INFO 140104354305856] #progress_notice. epoch: 1, iterations: 80 speed: 55.5808416362 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:58:40 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 1, train loss: 1.1833359790439515 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:58:40 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 1, train throughput: 53.404653492 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:58:55 INFO 140104354305856] #progress_notice. epoch: 1, iterations: 20 speed: 23.6098881053 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:59:09 INFO 140104354305856] #progress_notice. epoch: 1, iterations: 40 speed: 23.3332049191 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:59:22 INFO 140104354305856] #progress_notice. epoch: 1, iterations: 60 speed: 23.6669988778 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:59:36 INFO 140104354305856] #progress_notice. epoch: 1, iterations: 80 speed: 23.6050133099 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:59:43 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 1, validation pixel_accuracy: 0.8416115409762875 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:59:43 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 1, validation mIOU: 0.47542417453454033 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:59:43 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 1, validation throughput: 23.5706831042 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:59:43 INFO 140104354305856] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:59:44 INFO 140104354305856] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1549292384.115699, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 1}, \"StartTime\": 1549292289.995001}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:59:53 INFO 140104354305856] #progress_notice. epoch: 2, iterations: 20 speed: 54.2613937919 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 14:59:58 INFO 140104354305856] #progress_notice. epoch: 2, iterations: 40 speed: 54.5702562268 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:00:04 INFO 140104354305856] #progress_notice. epoch: 2, iterations: 60 speed: 54.3109282462 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:00:10 INFO 140104354305856] #progress_notice. epoch: 2, iterations: 80 speed: 55.3183633533 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:00:13 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 2, train loss: 0.8573067284772521 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:00:13 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 2, train throughput: 53.2300989843 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:00:29 INFO 140104354305856] #progress_notice. epoch: 2, iterations: 20 speed: 23.7234221412 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:00:42 INFO 140104354305856] #progress_notice. epoch: 2, iterations: 40 speed: 23.3348032434 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:00:56 INFO 140104354305856] #progress_notice. epoch: 2, iterations: 60 speed: 23.803259942 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:01:09 INFO 140104354305856] #progress_notice. epoch: 2, iterations: 80 speed: 23.6074878267 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:01:16 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 2, validation pixel_accuracy: 0.8615255358522461 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:01:16 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 2, validation mIOU: 0.5447719439325285 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:01:16 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 2, validation throughput: 23.5474452718 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:01:16 INFO 140104354305856] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:01:17 INFO 140104354305856] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1549292477.114622, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 2}, \"StartTime\": 1549292384.115923}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:01:25 INFO 140104354305856] #progress_notice. epoch: 3, iterations: 20 speed: 53.8197029481 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:01:31 INFO 140104354305856] #progress_notice. epoch: 3, iterations: 40 speed: 54.2323212651 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:01:37 INFO 140104354305856] #progress_notice. epoch: 3, iterations: 60 speed: 54.6420150551 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:01:43 INFO 140104354305856] #progress_notice. epoch: 3, iterations: 80 speed: 55.2316782204 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:01:47 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 3, train loss: 0.7319274796729746 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:01:47 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 3, train throughput: 52.8731037902 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:02:02 INFO 140104354305856] #progress_notice. epoch: 3, iterations: 20 speed: 23.5972194825 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:02:16 INFO 140104354305856] #progress_notice. epoch: 3, iterations: 40 speed: 23.5740344705 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:02:29 INFO 140104354305856] #progress_notice. epoch: 3, iterations: 60 speed: 23.7838991866 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:02:43 INFO 140104354305856] #progress_notice. epoch: 3, iterations: 80 speed: 23.6790407127 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:02:50 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 3, validation pixel_accuracy: 0.8572338123322882 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:02:50 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 3, validation mIOU: 0.5219448396804709 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:02:50 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 3, validation throughput: 23.5500216567 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:02:50 INFO 140104354305856] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1549292570.895493, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 3}, \"StartTime\": 1549292477.114873}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:02:59 INFO 140104354305856] #progress_notice. epoch: 4, iterations: 20 speed: 54.3209075493 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:03:05 INFO 140104354305856] #progress_notice. epoch: 4, iterations: 40 speed: 54.4443467298 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:03:11 INFO 140104354305856] #progress_notice. epoch: 4, iterations: 60 speed: 54.6786112652 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:03:17 INFO 140104354305856] #progress_notice. epoch: 4, iterations: 80 speed: 54.9629798285 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:03:20 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 4, train loss: 0.9126770916896371 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:03:20 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 4, train throughput: 53.1138926081 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:03:36 INFO 140104354305856] #progress_notice. epoch: 4, iterations: 20 speed: 23.365505927 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:03:49 INFO 140104354305856] #progress_notice. epoch: 4, iterations: 40 speed: 23.1362326915 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:04:03 INFO 140104354305856] #progress_notice. epoch: 4, iterations: 60 speed: 23.5511346568 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:04:17 INFO 140104354305856] #progress_notice. epoch: 4, iterations: 80 speed: 23.5558053179 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:04:23 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 4, validation pixel_accuracy: 0.8665389263297519 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:04:23 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 4, validation mIOU: 0.5434116184122434 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:04:23 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 4, validation throughput: 23.3299040316 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:04:23 INFO 140104354305856] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1549292663.89315, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 4}, \"StartTime\": 1549292570.89592}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:04:33 INFO 140104354305856] #progress_notice. epoch: 5, iterations: 20 speed: 54.192643533 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:04:39 INFO 140104354305856] #progress_notice. epoch: 5, iterations: 40 speed: 53.7985186919 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:04:44 INFO 140104354305856] #progress_notice. epoch: 5, iterations: 60 speed: 54.1818363844 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:04:50 INFO 140104354305856] #progress_notice. epoch: 5, iterations: 80 speed: 55.1752788203 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:04:54 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 5, train loss: 0.8504556819452378 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:04:54 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 5, train throughput: 53.0298411928 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:05:09 INFO 140104354305856] #progress_notice. epoch: 5, iterations: 20 speed: 23.4616189246 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:05:23 INFO 140104354305856] #progress_notice. epoch: 5, iterations: 40 speed: 23.5188297511 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:05:37 INFO 140104354305856] #progress_notice. epoch: 5, iterations: 60 speed: 23.3989244193 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:05:50 INFO 140104354305856] #progress_notice. epoch: 5, iterations: 80 speed: 23.5660459186 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:05:58 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 5, validation pixel_accuracy: 0.876768734200208 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:05:58 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 5, validation mIOU: 0.5569332354995392 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:05:58 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 5, validation throughput: 23.3153184932 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:05:58 INFO 140104354305856] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:05:58 INFO 140104354305856] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1549292758.609309, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 5}, \"StartTime\": 1549292663.893484}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:06:07 INFO 140104354305856] #progress_notice. epoch: 6, iterations: 20 speed: 53.6083512537 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:06:13 INFO 140104354305856] #progress_notice. epoch: 6, iterations: 40 speed: 54.2293850505 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:06:19 INFO 140104354305856] #progress_notice. epoch: 6, iterations: 60 speed: 54.7302497604 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:06:24 INFO 140104354305856] #progress_notice. epoch: 6, iterations: 80 speed: 54.9505134042 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:06:28 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 6, train loss: 0.8086105626547239 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:06:28 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 6, train throughput: 52.7734553359 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:06:43 INFO 140104354305856] #progress_notice. epoch: 6, iterations: 20 speed: 23.5642089037 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:06:57 INFO 140104354305856] #progress_notice. epoch: 6, iterations: 40 speed: 23.5818461338 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:07:10 INFO 140104354305856] #progress_notice. epoch: 6, iterations: 60 speed: 23.6320035778 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:07:24 INFO 140104354305856] #progress_notice. epoch: 6, iterations: 80 speed: 23.498307715 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:07:31 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 6, validation pixel_accuracy: 0.87693903884241 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:07:31 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 6, validation mIOU: 0.5667868121899338 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:07:31 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 6, validation throughput: 23.5354341586 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:07:31 INFO 140104354305856] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:07:31 INFO 140104354305856] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1549292851.681722, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 6}, \"StartTime\": 1549292758.609542}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:07:40 INFO 140104354305856] #progress_notice. epoch: 7, iterations: 20 speed: 54.7633889409 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:07:46 INFO 140104354305856] #progress_notice. epoch: 7, iterations: 40 speed: 54.1434992412 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:07:52 INFO 140104354305856] #progress_notice. epoch: 7, iterations: 60 speed: 54.6072895578 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:07:58 INFO 140104354305856] #progress_notice. epoch: 7, iterations: 80 speed: 54.8330134482 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:08:01 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 7, train loss: 1.0272405856572415 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:08:01 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 7, train throughput: 53.2775784212 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:08:17 INFO 140104354305856] #progress_notice. epoch: 7, iterations: 20 speed: 23.7242608094 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:08:31 INFO 140104354305856] #progress_notice. epoch: 7, iterations: 40 speed: 23.5943240469 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:08:44 INFO 140104354305856] #progress_notice. epoch: 7, iterations: 60 speed: 23.5741669688 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:08:58 INFO 140104354305856] #progress_notice. epoch: 7, iterations: 80 speed: 23.4974191267 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:09:05 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 7, validation pixel_accuracy: 0.8793038414266238 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:09:05 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 7, validation mIOU: 0.562702887125503 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:09:05 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 7, validation throughput: 23.4256471021 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:09:05 INFO 140104354305856] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1549292945.851689, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 7}, \"StartTime\": 1549292851.681926}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:09:14 INFO 140104354305856] #progress_notice. epoch: 8, iterations: 20 speed: 53.9943840384 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:09:20 INFO 140104354305856] #progress_notice. epoch: 8, iterations: 40 speed: 53.6727498412 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:09:26 INFO 140104354305856] #progress_notice. epoch: 8, iterations: 60 speed: 54.7324815985 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:09:32 INFO 140104354305856] #progress_notice. epoch: 8, iterations: 80 speed: 54.6917123254 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:09:35 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 8, train loss: 0.8393795757585174 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:09:35 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 8, train throughput: 53.3636250651 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:09:51 INFO 140104354305856] #progress_notice. epoch: 8, iterations: 20 speed: 23.5560120271 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:10:04 INFO 140104354305856] #progress_notice. epoch: 8, iterations: 40 speed: 23.3670110437 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:10:18 INFO 140104354305856] #progress_notice. epoch: 8, iterations: 60 speed: 23.6143161972 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:10:32 INFO 140104354305856] #progress_notice. epoch: 8, iterations: 80 speed: 23.5891157113 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:10:39 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 8, validation pixel_accuracy: 0.8849336199642512 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:10:39 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 8, validation mIOU: 0.5876447801895507 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:10:39 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 8, validation throughput: 23.4787674227 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:10:39 INFO 140104354305856] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:10:40 INFO 140104354305856] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 9, \"sum\": 9.0, \"min\": 9}}, \"EndTime\": 1549293040.089353, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 8}, \"StartTime\": 1549292945.852053}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:10:48 INFO 140104354305856] #progress_notice. epoch: 9, iterations: 20 speed: 54.5343366543 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:10:54 INFO 140104354305856] #progress_notice. epoch: 9, iterations: 40 speed: 54.4295538583 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:11:00 INFO 140104354305856] #progress_notice. epoch: 9, iterations: 60 speed: 53.8940010488 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:11:06 INFO 140104354305856] #progress_notice. epoch: 9, iterations: 80 speed: 55.0598432109 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:11:10 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 9, train loss: 0.7687717548662635 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:11:10 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 9, train throughput: 52.7123851118 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:11:25 INFO 140104354305856] #progress_notice. epoch: 9, iterations: 20 speed: 23.3281679796 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:11:39 INFO 140104354305856] #progress_notice. epoch: 9, iterations: 40 speed: 23.3776988793 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:11:53 INFO 140104354305856] #progress_notice. epoch: 9, iterations: 60 speed: 23.7263074085 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:12:06 INFO 140104354305856] #progress_notice. epoch: 9, iterations: 80 speed: 23.6843306319 samples/sec\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:12:13 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 9, validation pixel_accuracy: 0.8863316369121341 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:12:13 INFO 140104354305856] #quality_metric. host: algo-1, epoch: 9, validation mIOU: 0.5936332814186249 .\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:12:13 INFO 140104354305856] #throughput_metric. host: algo-1, epoch: 9, validation throughput: 23.4500331472 samples/sec.\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:12:13 INFO 140104354305856] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:12:13 INFO 140104354305856] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1549293133.83494, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 9}, \"StartTime\": 1549293040.089587}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:12:13 WARNING 140104354305856] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:12:13 INFO 140104354305856] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[02/04/2019 15:12:14 INFO 140104354305856] Test data is not provided.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"totaltime\": {\"count\": 1, \"max\": 959591.3908481598, \"sum\": 959591.3908481598, \"min\": 959591.3908481598}, \"setuptime\": {\"count\": 1, \"max\": 10.603904724121094, \"sum\": 10.603904724121094, \"min\": 10.603904724121094}}, \"EndTime\": 1549293134.02329, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\"}, \"StartTime\": 1549292174.700461}\n",
      "\u001b[0m\n",
      "\n",
      "2019-02-04 15:12:18 Uploading - Uploading generated training model\n",
      "2019-02-04 15:13:00 Completed - Training job completed\n",
      "Billable seconds: 1056\n"
     ]
    }
   ],
   "source": [
    "ss_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosting\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same instance (or type of instance) that we used to train. Training is a prolonged and compute heavy job that require a different of compute and memory requirements that hosting typically do not. We can choose any sagemaker supported instance we want to host the model. In our case we chose the `ml.p3.2xlarge` instance to train, but we choose to host the model on the less expensive cpu instance, `ml.c5.xlarge`. The endpoint deployment can be accomplished as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_predictor = ss_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Now that the trained model is deployed at an endpoint that is up-and-running, we can use this endpoint for inference. To do this, let us download an image from [PEXELS](https://www.pexels.com/) which the algorithm has so-far not seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O test.jpg https://images.pexels.com/photos/242829/pexels-photo-242829.jpeg\n",
    "filename = 'test.jpg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL\n",
    "\n",
    "# resize image size for inference\n",
    "im = PIL.Image.open(filename)\n",
    "im.thumbnail([800,600],PIL.Image.ANTIALIAS)\n",
    "im.save(filename, \"JPEG\")\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.imshow(im)\n",
    "plt.axis('off')\n",
    "with open(filename, 'rb') as image:\n",
    "    img = image.read()\n",
    "    img = bytearray(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The endpoint accepts images in formats similar to the ones found images in the training dataset. It accepts the `image/jpeg` `content_type`. \n",
    "\n",
    "The `accept` parameter takes on two values: \n",
    "\n",
    "  - `image/png`  - For segmentation mask as output\n",
    "  - `application/x-protobuf`. - For probabilities as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mask output\n",
    "ss_predictor.content_type = 'image/jpeg'\n",
    "ss_predictor.accept = 'image/png'\n",
    "return_img = ss_predictor.predict(img)\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "num_classes = 21\n",
    "mask = np.array(Image.open(io.BytesIO(return_img)))\n",
    "plt.imshow(mask, vmin=0, vmax=num_classes-1, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Probability output\n",
    "# resize image size for inference\n",
    "im = PIL.Image.open(filename)\n",
    "im.thumbnail([800,600],PIL.Image.ANTIALIAS)\n",
    "im.save(filename, \"JPEG\")\n",
    "with open(filename, 'rb') as image:\n",
    "    img = image.read()\n",
    "    img = bytearray(img)\n",
    "    \n",
    "ss_predictor.content_type = 'image/jpeg'\n",
    "ss_predictor.accept = 'application/x-protobuf'\n",
    "results = ss_predictor.predict(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we receive back is a recordio-protobuf of probablities sent as a binary. It takes a little bit of effort to convert into a readable array. Let us convert them to numpy format. We can make use of `mxnet` that has the capability to read recordio-protobuf formats. Using this, we can convert the outcoming bytearray into numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load them using Record format.\n",
    "from sagemaker.amazon.record_pb2 import Record\n",
    "import mxnet as mx\n",
    "\n",
    "results_file = 'results.rec'\n",
    "with open(results_file, 'wb') as f:\n",
    "    f.write(results)\n",
    "\n",
    "rec = Record()\n",
    "recordio = mx.recordio.MXRecordIO(results_file, 'r')\n",
    "protobuf = rec.ParseFromString(recordio.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The protobuf array has two parts to it. The first part contains the shape of the output and the second contains the values of probabilites. Using the output shape, we can transform the probabilities into the shape of the image, so that we get a map of values. There typically is a singleton dimension since we are only inferring on one image. We can also remove that using the `squeeze` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list(rec.features[\"target\"].float32_tensor.values)\n",
    "shape = list(rec.features[\"shape\"].int32_tensor.values)\n",
    "shape = np.squeeze(shape)\n",
    "mask = np.reshape(np.array(values), shape)\n",
    "mask = np.squeeze(mask, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as to plot the segmentation mask from the list of probabilities, let us get the indices of the most probable class for each pixel. We can do this by measuring the `argmax` across the classes axis of the probability data. To plot the probabilites as image, we can use the `numpy.argmax` method to find out which probabilities are the largest and plot only those as a segmentaiton mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_map = np.argmax(mask, axis=0)\n",
    "num_classes = 21\n",
    "plt.imshow(pred_map, vmin=0, vmax=num_classes-1, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "Having an endpoint running will incur some costs. Therefore as a clean-up job, we should delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(ss_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
